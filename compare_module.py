
import numpy as np
import matplotlib.pyplot as plt
import scipy.signal as sn
from scipy import stats as st
from sklearn.metrics import mean_absolute_error, mean_squared_error



## compare the similarity of 2 signals
def compare_for_trained_lstm(baseline_signal:np.ndarray, generated_signal:np.ndarray, starting_point:int, fs):
    """This function takes as input the forecasted signal and compares it with the actual follwoing signal in order to measure the similarity of the two signals. The 
    greater the similarity of the two signals, the most accurate will be the forecasting (ideally the forecasted signal would be completely the same as the actual (baseleine)
    signal. The variable 'generated signal' is the forecasted signal that has been generated by the ML model. The "baseline_signal" is the actual signal that followed and
    'fs' is the sampling_frequency of the baseline_signal)
    Stating point is the point in which forecasting starts. It is importana because before the starting point signals are the same and souldn't be compared because they
    falsely give better comparing results. However the previus parts are ploted, because they give a better optical impression of the forecasting, and that is the reason
    they weren't ommited completely from this function"""

    pure_gen_signal = generated_signal[starting_point:] # με αυτή και την επόμενη γραμμή παίρνουμε τα σήματα χώρίς το input signal που χρησιμοποιηθηκε για την παραγωγη του
    pure_base_signal = baseline_signal[starting_point:] # σήματος και επειδή είναι το ίδιο και στα δύο, αλλοιώνει τις μετρικές, βελτιώνοντας τις
    
    MAE = mean_absolute_error(pure_base_signal, pure_gen_signal)
    print ('Mean absolute error is', MAE)
    RMSE = np.sqrt(mean_squared_error(pure_base_signal, pure_gen_signal))
    print ('Root mean square error is', RMSE)
    np_correlate = np.correlate(pure_base_signal/pure_base_signal.std(), pure_gen_signal/pure_gen_signal.std())/len(pure_base_signal) # η μορφή αυτή είναι κανονικοποιήμένη
    print('Cross-correlation (np.correlate) is', np_correlate) # η cross-correlation ΕΙΝΑΙ κανονικοποιήμένη στο [-1,1]
    np_corrcoef = np.corrcoef(pure_base_signal, pure_gen_signal)[0,1]
    print('Cross-correlation (np.corrcoef) is', np_corrcoef) # takes values in [-1,1]. The closer to 1 the greter the similarity. Is a Pearson corealation coeficient so is almost the same as st.pearsonr
    # sn_corr = sn.correlate(signal_baseline/signal_baseline.std(), gen_signal/gen_signal.std())/len(signal_baseline) # παραλλαγή του από κάτω
    sn_corr = sn.correlate(pure_base_signal, pure_gen_signal)/(pure_base_signal.std()*pure_gen_signal.std()*len(pure_base_signal))
    plt.plot(sn_corr); plt.title('sn.correlate cross correlation'); plt.show() # η corss-correlation ΕΙΝΑΙ κανονικοποιήμένη στο [-1,1]
    pearson_r = st.pearsonr(pure_base_signal, pure_gen_signal)
    print('Pearson (Cross-)correlation (st.pearsonr) is', pearson_r) # takes values in [-1,1]. The closer to 1 the greter the similarity, and the closer is to 0 the less the similarity.
    spearman_rho = st.spearmanr(pure_base_signal, pure_gen_signal)
    print('Spearman rho correlation (st.spearmanr)) is', spearman_rho)


    # οπτίκή σύγκριση των δύο σημάτων περιλαμβάνοντας τα προηγούμενα τμήματα που παρήγαγαν το σήμα
    plt. plot(baseline_signal, label = 'original signal')
    plt. plot(generated_signal, label = 'generated signal')
    plt.legend()
    plt.title('Visual comparison with precedent signal')
    plt.show()
    plt.close()

    # οπτίκή σύγκριση μόνο του generated segment με το actual segment
    plt. plot(pure_base_signal, label = 'original signal')
    plt. plot(pure_gen_signal, label = 'generated signal')
    plt.legend()
    plt.title('Visual comparison of generated and actual segments')
    plt.show()
    plt.close()

    # για σύγκριση σήματων με διαφορετικό μέγεθός ή διαφορετικο downsampling θα μπορούσε να χρησιμοποιηθεί το dynamic time warping

    # compare signal frequencies (to see if generated signal have approximately the same spectral components)
    f1, Pxx_1 = sn.periodogram(pure_base_signal, fs=fs, return_onesided=True, scaling='density')
    f2, Pxx_2 = sn.periodogram(pure_gen_signal, fs=fs, return_onesided=True, scaling='density')
    plt.plot(f1,Pxx_1, label = 'original signal')
    plt.plot(f2,Pxx_2, label = 'generated signal')
    plt.suptitle('compare signals in time-domain (Fourier)')
    plt.title(f'sampling frequency is {fs} due to downsampling', fontsize = 9)
    plt.legend()
    plt.show()


#------------------------------------------------------------------------------------------------------------------------------------------------------------


import sklearn.linear_model as lr
from sklearn.dummy import DummyRegressor
import signal_handler
from sklearn import model_selection


norm_method = 'None'
def main1():
    import fc_LSTM

    input_size =1000
    output_size =100
    tag ='All_EA'
    downsample_scale = 10000
    scaler1 = signal_handler.lfp_scaler(scaling_power=4)
    norm_method = 'None'

    x_data, y_data, scaler1  = fc_LSTM.prepare_data(downsample_scale, extract_data =1, tag=tag, numpy_data_float32 =0, scaler = scaler1 , input_size=input_size, 
                                            output_size = output_size,  window_step =1, return_loaders = 0, cut_with_numpy = 1)
    x_data, y_data = x_data.numpy(), y_data.numpy()
    x_data=np.reshape(x_data, (x_data.shape[0]*x_data.shape[1], x_data.shape[2]))
    y_data=np.reshape(y_data, (y_data.shape[0]*y_data.shape[1], y_data.shape[2]))
    x_train, x_test, y_train, y_test = model_selection.train_test_split(x_data, y_data, train_size=0.9)

    test_time_series = signal_handler.time_series('test1', downsample_scale)
    print('test_time_series shape is', test_time_series.shape)
    starting_point = 1000
    num_gen_points = 200
    scaling_method = 'None'
    dummy(x_train, y_train, x_test, y_test, test_time_series, starting_point, num_gen_points, scaling_method)
    linear_autoregression(x_train, y_train, x_test, y_test, test_time_series, starting_point, num_gen_points, scaling_method)

#------------------------------------------------------------------------------------------------------------------------------------------------------------

def dummy(x_train, y_train, x_test, y_test, test_time_series, starting_point:int, num_gen_points:int, scaling_method:str):
    dummy = DummyRegressor(strategy='mean')
    dummy.fit(x_train, y_train)
    print('Dummy regresson R^2 score is ', dummy.score(x_test, y_test))
    pred = dummy.predict(x_test[0].reshape(1,-1))

    input_size = x_train.shape[1]
    output_size = y_train.shape[1]
    try_lfp = test_time_series[0]
    scaler = signal_handler.lfp_scaler(scaling_power=4)
    try_lfp_norm = scaler.fit_transform1d(try_lfp, scaling_method) # κανονικοποίηση του σήματος. Μια πολύ μιρκή λαθροχειρία είναι ότι η μεση τιμή εξάγεται και από το σήμα που θα προβλεφθεί, οπότε λαμβάνεται μια πληροφορία από την πρόβλεψη
    gen_signal = generate_lfp(dummy, try_lfp_norm[:starting_point], input_size, output_size, num_gen_points, scaling_method, only_generated = 0)
    gen_signal_denorm = scaler.inverse1d(gen_signal, scaling_method)
    fs = 1/(test_time_series[1,3] - test_time_series[1,2])
    compare_for_trained_lstm(try_lfp[:len(gen_signal)], gen_signal_denorm, starting_point, fs)


def linear_autoregression(x_train, y_train, x_test, y_test, test_time_series, starting_point, num_gen_points, scaling_method):
    linear = lr.LinearRegression()
    linear.fit(x_train, y_train)
    print('Dummy regresson R^2 score is ', linear.score(x_test, y_test))
    pred = linear.predict(x_test[0].reshape(1,-1))
    
    input_size = x_train.shape[1]
    output_size = y_train.shape[1]
    try_lfp = test_time_series[0]
    scaler = signal_handler.lfp_scaler(scaling_power=4)
    try_lfp_norm = scaler.fit_transform1d(try_lfp, scaling_method) # κανονικοποίηση του σήματος. Μια πολύ μιρκή λαθροχειρία είναι ότι η μεση τιμή εξάγεται και από το σήμα που θα προβλεφθεί, οπότε λαμβάνεται μια πληροφορία από την πρόβλεψη
    gen_signal = generate_lfp(linear, try_lfp_norm[:starting_point], input_size, output_size, num_gen_points, scaling_method, only_generated = 0)
    gen_signal_denorm = scaler.inverse1d(gen_signal, scaling_method)
    fs = 1/(test_time_series[1,3] - test_time_series[1,2])
    compare_for_trained_lstm(try_lfp[:len(gen_signal)], gen_signal_denorm, starting_point, fs)


def generate_lfp(model, signal:np.ndarray, input_size:int, output_size:int, num_gen_points:int, scaling_method:str, only_generated:bool):
    num_predictions =  int(num_gen_points/output_size) + 1
    # scaling_power = 6
    # signal = signal*(10**scaling_power)
    signal, _ = signal_handler.normalize_signal(signal.reshape(-1, 1), method= scaling_method, direction = 'normalize', scaler= 'None', scaling_power = 3) 
    input_signal = signal[(len(signal)-input_size):]
    _, denorm_scaler = signal_handler.normalize_signal(input_signal[:output_size].reshape(-1, 1), method= scaling_method, direction = 'normalize', scaler= 'None', scaling_power = 3)
    input_signal = input_signal.reshape(1,-1) # πρέπει να είναι σε αυτή τη μορφή για να εισαχθεί στο predict
    if only_generated: generated_signal = []
    if not(only_generated): generated_signal= list(signal)

    for i in np.arange(num_predictions):
        new_pred = model.predict(input_signal)
        new_pred = signal_handler.normalize_signal(new_pred.reshape(-1, 1), method= scaling_method, direction = 'inverse', scaler= denorm_scaler, scaling_power = 3)
        if new_pred.shape == (): new_pred = new_pred[np.newaxis]; 
        else: new_pred = new_pred[np.newaxis,:]
        if new_pred.shape == (1,): new_pred = new_pred.reshape(1,1) # για output = 1 
        input_signal = np.hstack((input_signal[:,output_size:], new_pred))  # θα ενώσω τα δύο κομμάτια μετακινόντας το κάποιες θέσεις
        if new_pred.shape == (1,1): generated_signal.append(np.squeeze(new_pred)) # για output = 1 
        else: generated_signal = generated_signal + list(np.squeeze(new_pred))
    generated_signal = np.array(generated_signal)
    if not(only_generated): generated_signal =  generated_signal[: len(signal)+num_gen_points]
    if only_generated: generated_signal =  generated_signal[:num_gen_points]
    return generated_signal #*(10**(-scaling_power))

# main1()



#------------------------------------------------------------------------------------------------------------------------------------------------------------
import colorednoise as cn


def main2():
    downsample_scale = 10
    test_time_series = signal_handler.time_series('test1', downsample_scale)
    fs = 1/(test_time_series[1,3] - test_time_series[1,2])
    compare_noise(test_time_series[0,:], fs, downsample_scale)


def bandpass(data: np.ndarray, edges: list[float], sample_rate: float, poles: int = 5):
    sos = sn.butter(poles, edges, 'bandpass', fs=sample_rate, output='sos')
    filtered_data = sn.sosfiltfilt(sos, data)
    return filtered_data


def produce_noise(beta, samples, fs, freq_range):
    # beta ->  the exponent  [1.f^(beta)] -> beta = 0 => white noise, beta = 1 => pink noise, beta = 2 => brownian noise
    # samples -> number of samples to generate
    noise = cn.powerlaw_psd_gaussian(beta, samples)
    noise = bandpass(noise, freq_range, fs, poles = 5)
    return noise


def compare_noise(baseline_signal, fs, downsample_scale):
    print('fs is', fs)
    samples = baseline_signal.shape[0]
    scaling = -3

    print('\n COMPARE SIGNAL WITH WHITE NOISE')
    white = produce_noise (beta = 0, samples =samples, fs=fs, freq_range = [0.001, 2000/downsample_scale])# for downsampling =10, the noise is filtered in the range of LFP [0,200] Hz
    white = white * (baseline_signal.std()/white.std()) # bring the noise to the same scale as the signal
    #white = white * np.median(baseline_signal) # because signal is irregular with many outliers, instead od .std(), .median() could be used for noise scaling ->it does not work
    compare_for_trained_lstm (baseline_signal, white, starting_point = 0, fs =fs)

    ### Actually LFP signal' PSD follows pink noise morphology
    print('\n COMPARE SIGNAL WITH PINK NOISE')
    pink = produce_noise (beta = 1, samples =samples, fs=fs, freq_range = [0.001, 2000/downsample_scale])# for downsampling =10, the noise is filtered in the range of LFP [0,200] Hz
    plt.plot(pink); plt.show()
    pink = pink * (baseline_signal.std()/pink.std()) # bring the noise to the same scale as the signal
    #pink = pink * baseline_signal.median() # because signal is irregular with many outliers, instead od .std(), .median() could be used for noise scaling ->it does not work
    compare_for_trained_lstm (baseline_signal, pink, starting_point = 0, fs =fs)


    print('\n COMPARE SIGNAL WITH BROWNINAN NOISE')
    brownian = produce_noise (beta = 2, samples =samples, fs=fs, freq_range = [0.001, 2000/downsample_scale]) # for downsampling =10, the noise is filtered in the range of LFP [0,200] Hz
    plt.plot(brownian); plt.show()
    brownian = brownian * (baseline_signal.std()/brownian.std()) # bring the noise to the same scale as the signal
    # brownian = brownian * baseline_signal.median() # because signal is irregular with many outliers, instead od .std(), .median() could be used for noise scaling ->it does not work
    compare_for_trained_lstm (baseline_signal, brownian, starting_point = 0, fs =fs)


if  __name__ == "__main__":
    main1()
#------------------------------------------------------------------------------------------------------------------------------------------------------------



